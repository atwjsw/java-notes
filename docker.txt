docker

- 查看端口状态： netstat -na | grep 8080
- 运行nginx image(-d 后台 -p 端口映射）: sudo docker run -d -p 8080:80 nginx
- 查看容器内部情况： sudo docker exec -it 7b bash
- /etc/nginx
- mkdir -p ~/nginx/www ~/nginx/logs ~/nginx/conf 
- sudo docker run -p 80:80 --name mynginx -v $PWD/www:/www -v $PWD/conf:/etc/nginx -v $PWD/logs:/wwwlogs  -d nginx
- sudo docker run -p 80:80 -v $PWD/www:/www -v $PWD/conf:/etc/nginx -v $PWD/logs:/wwwlogs  -d nginx
- sudo docker run -p 80:80 -v $PWD/conf/nginx.conf:/etc/nginx/nginx.conf -d nginx
- sudo docker run -p 80:80 -v $PWD/conf/nginx.conf:/etc/nginx/nginx.conf -v $PWD/conf/vhost:/etc/nginx/vhost -d nginx


COPY conf/nginx.conf /etc/nginx/nginx.conf
COPY conf/vhost/* /etc/nginx/vhost/*
sudo docker build -t custom-nginx .
sudo docker run -p 80:80 --name my-custom-nginx-container -d custom-nginx

sudo docker run -it -p 80:80  -v `pwd`/logs:/var/log/nginx -d nginx

docker kill $(docker ps -q) ; 
docker rm $(docker ps -a -q) //Delete all docker containers
docker rmi $(docker images -q) //Delete all docker images

菜鸟教程
1） Docker 架构
1.1） Docker 使用客户端-服务器 (C/S) 架构模式，使用远程API来管理和创建Docker容器。
1.2） Docker 容器通过 Docker 镜像来创建。 容器与镜像的关系类似于面向对象编程中的对象与类
1.3） 核心概念
1.3.1）镜像(Images)是创建 Docker 容器的模板。
1.3.2）容器(Container)是独立运行的一个或一组应用。
1.3.3）客户端(Client)通过命令行或者其他工具使用 Docker API (https://docs.docker.com/reference/api/docker_remote_api) 与 Docker 的守护进程通信。
1.3.4）主机(Host) 一个物理或者虚拟的机器用于执行 Docker 守护进程和容器。
1.3.5）仓库(Registry)用来保存镜像，可以理解为代码控制中的代码仓库。
1.3.6）Hub(https://hub.docker.com) 提供了庞大的镜像集合供使用。
1.3.7）Docker Machine是一个简化Docker安装的命令行工具，通过一个简单的命令行即可在相应的平台上安装Docker，比如VirtualBox、 Digital Ocean、Microsoft Azure。

What is a Container?
1) Containers are a way to package software in a format that can run isolated on a shared operating system. 
2) Unlike VMs, containers do not bundle a full operating system - only libraries and settings required to make the software work are needed. 
3) This makes for efficient, lightweight, self-contained systems and guarantees that software will always run the same, regardless of where it’s deployed.

# Lab01 Hello World
1) $ docker container run hello-world
2) generate this message, Docker took the following steps:
- The Docker client contacted the Docker daemon.
- The Docker daemon pulled the "hello-world" image from the Docker Hub.
- The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading.
- The Docker daemon streamed that output to the Docker client, which sent it to your terminal.
3) docker run -it ubuntu bash // interacitve mode

# lab02 First Alpine Linux Container
1.0) Running your first container
1.0.1) docker image pull alpine //fetches the alpine image from the Docker registry and saves it in our system.
1.0.2) docker image ls //see a list of all images on your system.

1.1) Docker Container Run
1.1.1) docker container run alpine ls -l  //run a Docker container based on this image.
- When you call run, the Docker client finds the image (alpine in this case), creates the container and then runs a command in  that container. 
- When you run "docker container run alpine", you provided a command (ls -l), so Docker started the command specified and you saw the listing.
1.1.3) docker container run alpine echo "hello from alpine"
- the Docker client ran the echo command in our alpine container and then exited it. 
- If you’ve noticed, all of that happened pretty quickly. 
- Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast!
1.1.5) docker container run alpine /bin/sh
1.1.6) docker container run -it alpine /bin/sh //run in an interactive terminal
1.1.7) docker container ls //shows you all containers that are currently running. 
1.1.8) docker container ls -a //a list of all containers that you ran
1.1.9) docker container run --help to see a list of all flags it supports. 

1.2) Terminology
1.2.1) Images 
- The file system and configuration of our application which are used to create containers. 
- To find out more about a Docker image, run docker image inspect alpine. 
- In the demo above, you used the docker image pull command to download the alpine image. 
- When you executed the command docker container run hello-world, it also did a docker image pull behind the scenes to download the hello-world image.
1.2.2) Containers - Running instances of Docker images, containers run the actual applications. 
- A container includes an application and all of its dependencies. 
- It shares the kernel with other containers, and runs as an isolated process in user space on the host OS. 
- You created a container using docker run which you did using the alpine image that you downloaded. 
- A list of running containers can be seen using the docker container ls command.
1.2.3) Docker daemon - The background service running on the host that manages building, running and distributing Docker containers.
1.2.4) Docker client - The command line tool that allows the user to interact with the Docker daemon.
1.2.5) Docker Store - Store is a registry of Docker images. You can think of the registry as a directory of all available Docker images. You’ll be using this later in this tutorial.

# Lab03 Simple Web App

2.1) Run a static website in a container
2.1.1) docker container run -d seqvence/static-site //download and run the image directly in one go
- a single-page website that was already created for this demo and is available on the Docker Store as seqvence/static-site.
- the image doesn’t exist on your Docker host, the Docker daemon first fetches it from the registry and then runs it as a container.
2.1.2) docker container ls  //view the running containers.
2.1.3) docker container stop 6b94e5e7b8a2
2.1.4) docker container rm 6b94e5e7b8a2
2.1.5) docker container run --name static-site -e AUTHOR="Daniel Wen" -d -P seqvence/static-site //launch a container in detached mode: 
-d will create a container with the process detached from our terminal
-P will publish all the exposed container ports to random ports on the Docker host
-e is how you pass environment variables to the container
--name allows you to specify a container name
AUTHOR is the environment variable name and Your Name is the value that you can pass
2.1.6) docker container port static-site	//see the ports by running
443/tcp -> 0.0.0.0:32768
80/tcp -> 0.0.0.0:32769 
- http://127.0.0.1:32769  //access web page
2.1.7) docker container run --name static-site-2 -e AUTHOR="Your Name" -d -p 8888:80 seqvence/static-site // "-p 8888:80" = publish to designed port 8888
2.1.8) docker container port static-site-2	
2.1.9) docker container stop static-site; docker container rm static-site //stop and remove 
2.2.0) docker container rm -f static-site-2 //a shortcut to remove container

2.2) Docker Images
2.2.1) docker image ls //to see the list of images that are available locally on your system
REPOSITORY             TAG                 IMAGE ID            CREATED             SIZE
seqvence/static-site   latest              92a386b6e686        2 hours ago        190.5 MB
nginx                  latest              af4b3d7d5401        3 hours ago        190.5 MB
python                 2.7                 1c32174fd534        14 hours ago        676.8 MB
postgres               9.4                 88d845ac7a88        14 hours ago        263.6 MB
containous/traefik     latest              27b4e0c6b2fd        4 days ago          20.75 MB
node                   0.10                42426a5cba5f        6 days ago          633.7 MB
redis                  latest              4f5f397d4b7c        7 days ago          177.5 MB
mongo                  latest              467eb21035a8        7 days ago          309.7 MB
alpine                 3.3                 70c557e50ed6        8 days ago          4.794 MB
java                   7                   21f6ce84e43c        8 days ago          587.7 MB
- The TAG refers to a particular snapshot of the image and the ID is the corresponding unique identifier for that image.
- think of an image akin to a git repository - images can be committed with changes and have multiple versions. 
- When you do not provide a specific version number, the client defaults to latest
2.2.2) docker image pull ubuntu:12.04 // pull a specific version of ubuntu image
2.2.3) docker image pull ubuntu	// default to a version named latest
2.2.4) To get a new Docker image you can either get it from a registry (such as the Docker Cloud) or create your own.
2.2.5) docker search //search for images directly from the command line 
2.2.6) base images vs. child images
- Base images are images that have no parent images, usually images with an OS like ubuntu, alpine or debian.
- Child images are images that build on base images and add additional functionality.
2.2.7) official images vs. user images. (Both of which can be base images or child images.)
2.2.7.1) Official images are Docker sanctioned images. 
- These are not prefixed by an organization or user name. In the list of images above, the python, node, alpine and nginx images are official (base) images. 
2.2.7.2)
- User images are images created and shared by users like you. 
- They build on base images and add additional functionality. 
- Typically these are formatted as user/image-name. The user value in the image name is your Docker Cloud user or organization name.

2.3) Create your first image
2.3.1 Create a Python Flask app that displays random cat pix
...
2.3.2) Write a Dockerfile
- all user images are based on a base image. Since our application is written in Python, we will build our own Python image based on Alpine. 
- A Dockerfile is a text file that contains a list of commands that the Docker daemon calls while creating an image. 
- The Dockerfile contains all the information that Docker needs to know to run the app — a base Docker image to run from, location of your project code, any dependencies it has, and what commands to run at start-up. 
- Dockerfile is a simple way to automate the image creation process. The best part is that the commands you write in a Dockerfile are almost identical to their equivalent Linux commands. This means you don't really have to learn new syntax to create your own Dockerfiles.
2.3.2.1) Create a file called Dockerfile, and add content to it as described below.
FROM alpine:3.5	// specifying our base image
2.3.2.2) install the Python pip package to the alpine linux distribution. This will not just install the pip package but any other dependencies too, which includes the python interpreter.
RUN apk add --update py2-pip
2.3.2.3) add the files that make up the Flask Application.
COPY requirements.txt /usr/src/app/
RUN pip install --no-cache-dir -r /usr/src/app/requirements.txt
COPY app.py /usr/src/app/
COPY templates/index.html /usr/src/app/templates/
2.3.2.4) Specify the port number which needs to be exposed. 
EXPOSE 5000
2.3.2.5) running the application
CMD ["python", "/usr/src/app/app.py"]
2.3.2.6) Verify your Dockerfile.
cat Dockerfile

2.3.3) Build the image
docker build -t atwjsw/myfirstapp . 
- an optional tag name with the -t flag, and 
- the . indicates the current directory containing the Dockerfile 

2.3.4 Run your image
docker container run -p 8888:5000 --name myfirstapp atwjsw/myfirstapp
localhost:8888
docker container rm -f myfirstapp

2.3.5) Dockerfile commands summary
2.3.5.1) FROM starts the Dockerfile. 
- Dockerfile must start with the FROM command. 
- Images are created in layers, which means you can use another image as the base image for your own. 
- The FROM command defines your base layer. 
- it takes the name of the image As arguments.
- Optionally, you can add the Docker Cloud username of the maintainer and image version, in the format username/imagename:version.

2.3.5.2) RUN is used to build up the Image you’re creating. 
- For each RUN command, Docker will run the command then create a new layer of the image. 
- This way you can roll back your image to previous states easily. 
- The syntax for a RUN instruction is to place the full text of the shell command after the RUN (e.g., RUN mkdir /user/local/foo). 
- This will automatically run in a /bin/sh shell. You can define a different shell like this: RUN /bin/bash -c 'mkdir /user/local/foo'

2.3.5.3) COPY copies local files into the container.

2.3.5.4) CMD defines the commands that will run on the Image at start-up. 
- Unlike a RUN, this does not create a new layer for the Image, but simply runs the command. 
- There can only be one CMD per a Dockerfile/Image. 
- If you need to run multiple commands, the best way to do that is to have the CMD run a script. 
- CMD requires that you tell it where to run the command, unlike RUN. So example CMD commands would be: 
CMD ["python", "./app.py"]
CMD ["/bin/bash", "echo", "Hello World"]

2.3.5.5) EXPOSE opens ports in your image to allow communication to the outside world when it runs in a container.

#Lab04 Docker images deeper dive
1) Image creation from a container
1.1) docker container run -ti ubuntu bash  //running an interactive shell in a ubuntu container.
1.2) apt-get update; apt-get install -y figlet //install the figlet package in this container. 
1.3) exit //exit from this container
1.4) docker container ls -a // Get the ID of this container
1.5) docker container commit aee9421039ac //commit the container and create an image out of it
1.6) docker image ls //see the newly created image
1.7) docker image tag 9d58b21715f3 ourfiglet //get the ID of the newly created image and tag it as ourfiglet.
1.8) docker container run ourfiglet figlet hello // run a container based on the newly created image named ourfiglet, and specify the command to be ran such as it uses the figlet package

1.9) This example shows that we can create a container, add all the libraries and binaries in it and then commit this one in order to create an image. 
1.10) We can then use that image as we would do for any other images. This approach is not the recommended one as it is not very portable.

2) Image creation using a Dockerfile

# build from official linux image
2.1) a Dockerfile is a text file that contains all the instructions to build an image.
2.2) build a hello world application in Node.js
2.3) Copy the following content into index.js file.
var os = require("os");
var hostname = os.hostname();
console.log("hello from " + hostname);
2.4) plan: We will use alpine as the base image, add a Node.js runtime and then copy our source code. We will also specify the default command to be ran upon container creation.
2.5) create Dockerfile 
FROM alpine    //use alpine as the base image
RUN apk update && apk add nodejs //add a Node.js runtime
COPY . /app // copy our source code
WORKDIR /app
CMD ["node","index.js"] //default command to be ran
2.5) docker image build -t hello:v0.1 .  //build from current directory, REPOSITARY is hello, TAG is v0.1
2.6) docker container run hello:v0.1  //create container from hello:v0.1 with random assigned name
hello from 853b9507d395

# build from a user custom image
2.7) There are always several ways to write a Dockerfile, we can start from a Linux distribution and then install a runtime (as we did above) or use images where this has already been done for us.
2.8) we will use the mhart/alpine-node:6.9.4 image. This is not an official image but it’s a very well known and used one.
2.9) Create a new Dockerfile named Dockerfile-v2
FROM mhart/alpine-node:6.9.4
COPY . /app
WORKDIR /app
CMD ["node","index.js"]
2.10)  In this example, installing Node.js is not a big deal, but it is really helpful to use image where a runtime (or else) is already packages when using more complex environments.
2.11) docker image build -f Dockerfile-v2 -t hello:v0.2 .   //use the -f option to point towards the dockerfile
2.12) docker container run hello:v0.2
hello from 2213f7b29665

# ENTRYPOINT vs COMMAND 
1)  Dockerfile-v3
FROM alpine
ENTRYPOINT ["ping"]
CMD ["localhost"]
2) define the ping command as the ENTRYPOINT and the localhost as the CMD
3) the command that will be ran by default is the concatenation of ENTRYPOINT and CMD: ping localhost.
4) This command can be seen as a wrapper around the ping utility to which we can change the address we provide as a parameter.
5) docker image build -f Dockerfile-v3 -t ping:v0.1 .	//ping localhost
6) docker container run ping:v0.1 8.8.8.8	//ping 8.8.8.8

# Image Inspection
1) docker image pull alpine //pull image
- docker image inspect alpine
- the layers the image is composed of
- the driver used to store the layers
- the architecture / os it has been created for metadata of the image
2) docker image inspect --format "{{ json .RootFS.Layers }}" alpine | python -m json.tool //  Go template notation that enables to extract the part of information
3) docker image inspect --format "{{ .Architecture }}" alpine  // query only the Architecture information

# Filesystem exploration
1) docker container stop $(docker container ls -aq) //stop all containers
2) docker container rm $(docker container ls -aq) //remove all containers
3) docker image rm $(docker image ls -q) //remove all the images
4) ls /graph/overlay2 //look at the /graph/overlay2 folder where the image and container layers are stored.
5) docker image pull nginx
Using default tag: latest
latest: Pulling from library/nginx
5040bd298390: Pull complete
d7a91cdb22f0: Pull complete
9cac4850e5df: Pull complete
Digest: sha256:33ff28a2763feccc1e1071a97960b7fef714d6e17e2d0ff573b74825d0049303
Status: Downloaded newer image for nginx:latest
5.1) 3 layers are pulled.
6) ls /graph/overlay2
06055391cfdebd41ef0a35b7575286d62db95d4bc11042bab4b9c91d05b9825f
0709ac19f36f3d183788bf1f0fe33fc2e3118f3173a5655c9bc50bfa19319fc8
56d6b090394e2072c2ba18c0189d805bda56eb7bcdfb830c219b819e3623dc7c
6.1) Some folders, with names that looks like hash, were created. Those are the layers which, merged together, build the image filesystem.
7) docker container run -d nginx // run a container based on nginx image 
7.1) -d, --detach Run container in background and print container ID
8) ls /graph/overlay2 
06055391cfdebd41ef0a35b7575286d62db95d4bc11042bab4b9c91d05b9825f
0709ac19f36f3d183788bf1f0fe33fc2e3118f3173a5655c9bc50bfa19319fc8
378e9bcaa0f08581549eac2500ddb8fb422e387b87921d29f6107822466a05e6
378e9bcaa0f08581549eac2500ddb8fb422e387b87921d29f6107822466a05e6-init
56d6b090394e2072c2ba18c0189d805bda56eb7bcdfb830c219b819e3623dc7c
8.1) We can now see 2 additional folders (ID, ID-init), those ones correspond to the read-write layer of the running container.

# Lab#05 Docker containers deeper dive
1) Launch containers
1.1) Running a container in foreground
- docker container run -ti alpine
- docker container run -ti alpine sh

1.2) Running a container in background: Very often, containers are ran in background. They can expose services like HTTP API, databases, ...
1.2.1) docker container run -d --name mongo mongo:3.2 
- run a container in background (using the -d option).
- we have also used the –name option to assign a name to the container.
1.2.2) docker container exec -ti mongo bash // use the name of the container or the ID returned by the previous command and jump into the running container.
- We are now in the container, that can be really handy for debugging purposes sometimes. e.g. ps ax

1.3)Inspection of a container
1.3.1) docker container run --name www -d nginx
1.3.2) docker container inspect www
1.3.3) use the Go template notation to get only the information we are interested in:
docker container inspect --format "{{ .Config.Hostname }}" www
docker container inspect --format "{{ .NetworkSettings.IPAddress }}" www

1.4) docker container --help //get other commands

# Understand the container layer
1) The container layer is the layer created when a container is run. This is the layer in which the changes applied are stored. 
2) This layer is deleted when the container is removed and thus cannot be used for persistent storage.
3) In fact, this new ubuntu container is different from the previous one, the one in which figlet was installed. Both containers have their own container’s layer. Remember, the container’s layer is the read-write layer created when a container is run, it’s the place where changes done within the container are saved.

1.6) Cleanup
- docker container ls -a
- docker container ls -aq // -q option get us only the ID of the container.
- docker container rm -f $(docker container ls -aq) //remove several containers at the same time as we can feed the rm command with this list of ids.

1.7) container layer, the read-write layer that is added to each container that is ran. 
1.8) container API : run, exec, ls, rm, inspect

# Lab06 Docker Volumes
1) Data persistency without a volume ? Not possible
1.1) data is not persisted outside of a container by default.
1.2) docker container run --name c1 -ti alpine sh  //run an interactive shell within an alpine container named c1.
1.3) mkdir /data && cd /data && touch hello.txt //create the /data folder and a dummy hello.txt file in it
1.4) exit  //exit the container
1.5) docker container inspect c1 //check how the read-write layer (container layer) is accessible from the host.
- scroll into the output until the GraphDriver key
1.6) docker container inspect -f "{{ json .GraphDriver }}" c1 | python -m json.tool
{
    "Data": {
        "LowerDir": "/graph/overlay2/55922a6b646ba6681c5eca253a19e90270e3872329a239a82877b2f8c505c9a2-init/diff:/graph/overlay2/30474f5fc34277d1d9e5ed5b48e2fb979eee9805a61a0b2c4bf33b766ba65a16/diff",
        "MergedDir": "/graph/overlay2/55922a6b646ba6681c5eca253a19e90270e3872329a239a82877b2f8c505c9a2/merged",
        "UpperDir": "/graph/overlay2/55922a6b646ba6681c5eca253a19e90270e3872329a239a82877b2f8c505c9a2/diff",
        "WorkDir": "/graph/overlay2/55922a6b646ba6681c5eca253a19e90270e3872329a239a82877b2f8c505c9a2/work"
    },
    "Name": "overlay2"
}
1.7) ls /graph/overlay2/[YOUR_ID]/diff/data // inspect the folder which path is specified in UpperDir, we can see our /data and the hello.txt file we created are there.
1.8) docker container rm c1 //remove our c1 container now
1.9) data created in a container is not persisted. It’s removed with the container’s layer when the container is deleted.

2) Defining a volume in a Dockerfile
2.1) volumes come into the picture to handle the data persistency.
2.2) creating a Dockerfile based on alpine and define the /data as a volume. This means that anything written by a container in /data will be persisted outside of the Union filesystem.
2.3) Create a Dockerfile with the following content
FROM alpine
VOLUME ["/data"]
ENTRYPOINT ["/bin/sh"]
2.4) docker image build -t img1 . //build an image from this Dockerfile
2.5) docker container run --name c2 -ti img1 // create a container in interactive mode
2.6) From the shell we will go into /data and create a hello.txt file.
cd /data
touch hello.txt
ls
2.7) use the Control-P / Control-Q combination for exiting the container but making sure it remains running
2.8) docker container inspect c2 // inspect this container and find the Mounts key…
2.9) Or, 

2.10) a volume bypasses the union filesystem and is not dependent on a container’s lifecycle. docker container inspect -f "{{ json .Mounts }}"  c2 | python -m json.tool
[
    {
        "Destination": "/data",
        "Driver": "local",
        "Mode": "",
        "Name": "2f5b7c6b77494934293fc7a09198dd3c20406f05272121728632a4aab545401c",
        "Propagation": "",
        "RW": true,
        "Source": "/graph/volumes/2f5b7c6b77494934293fc7a09198dd3c20406f05272121728632a4aab545401c/_data",
        "Type": "volume"
    }
]
2.11) the volume defined in /data is stored in /graph/volumes/2f5…01c/_data on the host (removing part of the ID for a better readability).
2.12) docker container stop c2 && docker container rm c2 //remove the c2 container.
2.13) Check that the folder defined under the Source key is still there and contains hello.txt file.
ls /graph/volumes/24eea8c1950fb84580814cb19655b3ba562fbd81c5ed6f859f5427737c0a8d9d/_data

3) Defining a volume at runtime
3.1) volumn can also be defined at runtime using the -v flag of the docker container run command.
3.2） docker container run --name c3 -d -v /data alpine sh -c 'ping 8.8.8.8 > /data/ping.txt'
3.3） docker container inspect -f "{{ json .Mounts }}" c3 | python -m json.tool
3.4） tail -f /graph/volumes/OUR_ID/_data/ping.txt
- The ping.txt file is updated regularly by the command running in the c3 container.

4） Usage of the Volume API
4.1） docker volume --help
4.2） docker volume create --name html // create a volume named html.
DRIVER              VOLUME NAME
[other previously created volumes]
local               html
4.3） docker volume inspect html
[
    {
        "Driver": "local",
        "Labels": {},
        "Mountpoint": "/graph/volumes/html/_data",
        "Name": "html",
        "Options": {},
        "Scope": "local"
    }
]
- The Mountpoint defined here is the path on the Docker host where the volume can be accessed. - this path uses the name of the volume instead of the auto-generated ID we saw in the example above.
- We can now use this volume and mount it on a specific path of a container. 

4.4） docker container run --name www -d -p 8080:80 -v html:/usr/share/nginx/html nginx
- use a Nginx image and mount the html volume onto /usr/share/nginx/html folder within the container.
- Note: /usr/share/nginx/html is the default folder served by nginx. It contains 2 files: index.html and 50x.html
- use the -p option to map the nginx default port (80) to a port on the host (8080). 

4.5） docker container inspect xxxx
"Mounts": [
            {
                "Type": "volume",
                "Name": "html",
                "Source": "/graph/volumes/html/_data",
                "Destination": "/usr/share/nginx/html",
                "Driver": "local",
                "Mode": "z",
                "RW": true,
                "Propagation": ""
            }
        ]
4.6) ls /graph/volumes/html/_data // have a look at the content of the volume.
- The content of the /usr/share/nginx/html folder of the www container has been copied into the /graph/volumes/html/_data folder on the host.
4.7) Let’s have a look at the nginx’s welcome page
4.8) modify the index.html file and verify the changes are taken into account within the container.
4.9) have a look at the nginx’s welcome page. We can see the changes we have done in the index.html.
cat<<END >/graph/volumes/html/_data/index.html
SOMEONE HERE ?
END

5) Mount host’s folder into a container
5.1) named bind-mount and consist of mounting a host’s folder into a container’s folder. 
5.2) docker container run -v HOST_PATH:CONTAINER_PATH [OPTIONS] IMAGE [CMD]
- HOST_PATH and CONTAINER_PATH can be a folder or file. HOST_PATH must exist before running this command.

5.3) 1st case: the CONTAINER_PATH does not exist within the container
5.3.1) docker container run -ti -v /tmp:/data alpine sh //run an alpine container bind mounting the local /tmp folder inside the container /data folder.
-  By default, there is no /data folder in an alpine distribution. What is the impact of the bind-mount ?
5.3.2) In a shell inside our container: ls /data
- The /data folder has been created inside the container and it contains the content of the /tmp folder of the host. 
- We can now, from the container, change files on the host and the other way round.

5.4) 2nd case: the CONTAINER_PATH exists within the container
5.4.1) docker container run -ti -v /tmp:/usr/share/nginx/html nginx bash
//run a nginx container bind mounting the local /tmp folder inside the /usr/share/nginx/html folder of the containe
- Are the default index.html and 50x.html files still there in the container’s /usr/share/nginx/html folder ?
5.4.2) ls /usr/share/nginx/html
5.4.3) The content of the container’s folder has been overridden with the content of the host folder.

6) ***Bind-mounting is very usefull in development as it enables, for instance, to share source code on the host with the container.

# Lab07 Swarm mode introduction
1) this tutorial will show you how to setup a swarm and deploy your first services.
2) docker swarm init --advertise-addr $(hostname -i) // Init your swarm
- Swarm initialized: current node (x4rncinxcdcxlel55e16k8a49) is now a manager.
2.1) Copy the join command (watch out for newlines) output and paste it in the other terminal.
docker swarm join --token SWMTKN-1-1tnvwklvrepj1gbmag5wto8xdshoc543pmc0j5a45y6m9nww3l-2eiw7i5cikbewv262meb63b1k 10.0.146.3:2377

3) docker node ls // Show members of swarm, type the below command in the first terminal:
4) Creating services
4.1) docker service create -p 80:80 --name web nginx:latest
4.2) docker service ls //  list out the services
4.3) curl http://localhost:80 // check that nginx is running

4) Scaling up
4.1) docker service inspect web //inspect the service
4.2) docker service scale web=15 //scale the service:
4.3) docker service ps web // Docker has spread the 15 services evenly over all of the nodes

5) Updating nodes
5.1) docker node update --availability drain node2 //drain a particular node, that is remove all services from that node. 
- The services will automatically be rescheduled on other nodes.
5.2) docker node ls
- check out the nodes and see that node2 is still active but drained.

6) Scaling down
6.1) docker service scale web=10 // scale down the service
6.2) docker service ps web //check our service status
6.3) docker node update --availability active node2 //bring node2 back online
6.4) docker node inspect node2 --pretty // show it’s new availability

# Lab#08 Swarm stack introduction (swarm, stack, node, service, leader, worker)
1) The purpose of this lab is to illustrate how to deploy a stack (multi services application) against a Swarm using a docker compose file.

2) Init your swarm
2.1) docker swarm init --advertise-addr $(hostname -i)
2.2) copy the join command (watch out for newlines) and paste it in the other terminal.
2.3) docker node ls // Show members of swarm
2.4) Clone the voting-app
git clone https://github.com/docker/example-voting-app
cd example-voting-app
2.5) Deploy a stack
docker stack deploy --compose-file=docker-stack.yml voting_stack
- A stack is a group of services that are deployed together. 
- The docker-stack.yml in the current folder will be used to deploy the voting app as a stack.
- create a stack from a docker compose file is a great feature added in Docker 1.13.
2.6) docker stack ls //Check the stack deployed
2.7) docker stack services voting_stack //check the service within the stack
2.8) docker service ps voting_stack_vote //list the tasks of the vote service.

===============================================================================================
Docker in Action
# Chapter 2 Running software in containers
1) docker help
2) docker help container
3) docker run --detach --name web nginx:latest
4) docker run -d --name mailer dockerinaction/ch2_mailer
5) docker run --interactive --tty --link web:web --name web_test busybox:latest /bin/sh
- --interactive (or -i) and –-tty (or –t)
- linked to the container that’s running NGINX
6) wget -O - http://web:80/
-  wget to make an HTTP request to the web server
7) It’s possible to create an interactive container, manually start a process inside that
container, and then detach your terminal. You can do so by holding down the Crtl (or
Control) key and pressing P and then Q.
8) docker run -it --name agent --link web:insideweb --link mailer:insidemailer dockerinaction/ch2_agent
9) docker ps //check which containers are currently running
10) docker restart web // restart the containers
11) docker logs web //examine the logs for each container
11.1) Anything that the program writes to the stdout or stderr output streams will be recorded in this log. The problem with this pattern is that the log is never rotated or truncated, so the data written to the log for a container will remain and grow as long as the container
exists. That long-term persistence can be a problem for long-lived processes. A better
way to work with log data uses volumes and is discussed in chapter 4.
12) docker logs agents -f  //display the logs and then continue watching and updating the display
13) docker stop web
14) docker logs mailer
15) docker run -d --name namespaceA busybox:latest /bin/sh -c "sleep 30000"
16) docker exec namespaceA ps // run additional processes in a
running container. In this case the command you use is called ps, which shows all the
running processes and their PID.
17) docker run -d --name wp --read-only wordpress:4		//install worpress and run
18) docker inspect --format "{{.State.Running}}" wp
19) docker run -d --name wpdb -e MYSQL_ROOT_PASSWORD=ch2demo mysql:5 //install MySQL using Docker
20) docker run -d --name wp2 --link wpdb:mysql -p 80 --read-only wordpress:4 //Create a link
to the database
21) docker run -d --name wp3 --link wpdb:mysql -p 80 -v /run/lock/apache2/ -v /run/apache2/ --read-only wordpress:4 // Start the container with specific volumes for read only exceptions
22) docker run --env MY_ENVIRONMENT_VAR="this is a test" busybox:latest env
23) docker create \
--env WORDPRESS_DB_HOST=<my database hostname> \
--env WORDPRESS_DB_USER=site_admin \
--env WORDPRESS_DB_PASSWORD=MeowMix42 \
wordpress:4

CHapter 3 Software installation simplified
1) What is a repository? A repository is a named bucket of images
2) [REGISTRYHOST/][USERNAME/]NAME[:TAG]
3) docker pull quay.io/dockerinaction/ch3_hello_registry:latest
4) Distributing a Dockerfile with a project simplifies image builds on user
machines.
5) Images are usually related to other images in parent/child relationships. These
relationships form layers. When we say that we have installed an image, we are
saying that we have installed a target image and each image layer in its lineage.
6) Structuring images with layers enables layer reuse and saves bandwidth during
distribution and storage space on your computer.

Chapter 4 Persistent storage and shared state with volumes
1）需求场景1：When programs connect to the database and enter data, where is that data stored? Is it in a file inside the container? What happens to that data when you stop the container or remove it?
2) 需求场景2：Where would you write log files so that they will outlive the container? How would you get access to those logs to troubleshoot a problem?
3） A volume is a mount point on the container’s directory tree where a portion of the host directory tree has been mounted.
4） docker run -d --volume /var/lib/cassandra/data --name cass-shared alpine echo Data Container //creating a single container that defines a volume. This is called a volume container.
5）docker run -d --volumes-from cass-shared --name cass1 cassandra:2.2
- After Docker pulls the cassandra:2.2 image from Docker Hub, it creates a new container and copies the volume definitions from the volume container.
- After that, both containers have a volume mounted at /var/lib/cassandra/data that points to the same location on the host’s directory tree.
6） docker run -it --rm --link cass1:cass cassandra:2.2 cqlsh cass //start a container from the cassandra:2.2 image, but run a Cassandra client tool and connect to your running server
7） select * from system.schema_keyspaces where keyspace_name = 'docker_hello_world';
8）create keyspace docker_hello_world 
with replication = {
'class' : 'SimpleStrategy',
'replication_factor': 1
};
9）docker rm -vf cass1
10） Both the Cassandra client and server you created will be deleted after running those
commands. If the modifications you made are persisted, the only place they could
remain is the volume container.
11）If that is true, then the scope of that data has expanded to include two containers, and its life cycle has extended beyond the container where the data originated.If that is true, then the scope of that data has expanded to include two containers, and its life cycle has extended beyond the container where the data originated.
12） docker run -d --volumes-from cass-shared --name cass2 cassandra:2.2
13）docker run –it --rm --link cass2:cass cassandra:2.2 cqlsh cass
14）select * from system.schema_keyspaces where keyspace_name = 'docker_hello_world';
15）confirms the previous claims and demonstrates how volumes might be used to create durable systems.
16）docker run -d --name bmweb -v ~/example-docs:/usr/local/apache2/htdocs -p 80:80 httpd:latest
17) This example touches on an important attribute or feature of volumes. When you
mount a volume on a container file system, it replaces the content that the image provides
at that location. In this example, the httpd:latest image provides some default
HTML content at /usr/local/apache2/htdocs/, but when you mounted a volume at
that location, the content provided by the image was overridden by the content on the
host. This behavior is the basis for the polymorphic container pattern discussed later
in the chapter.
18) docker run --name bmweb_ro --volume ~/example-docs:/usr/local/apache2/htdocs/:ro -p 80:80 httpd:latest //mount volumes as read-only by appending :ro to the volume map specification


Introduction to Docker
1. what is Docker?
- Docker is a container for application, it is like a glass for water, a backpack for books. you can put application into docker, just like you can put water into a glass.
- Docker is developed in Go language and is open sourced.
- Docker supports multiple platforms, windows, MacOS, Linux, therefore, developer and package their application to a container and delpoy to different platforms.

2. Simple example

2. Docker vs. Cloud, Docker vs. Microservices

2. Phylosophy:
- Container: Without container, shipments may be misplaced or lost. Think about your deployment experiences, you want to deploy a Java Web App to a new environment. Very likey you may forget to setup some configuration, even forget to install a tool/infrastructure app.
- Standardization: delivery: publish to the repository/download docker repository (not by war file, by ftp...), storage: install to certian directory vs. standard installation/storage; API: standard command, same command for different app. 
- isolation: similar to virtual machine, based on Linux core restriction, LXC

3. Problems Solved:
3.1) Deployment issue: Ops: Man! your app doesn't work after we deploys it. Developer: No way, it ran smoothly in my environment.
e.g.  java web = os + jdk + tomcat + sources codes + configuration. (jdk version, tomcat version, configuration depends on local environment)
in docker, all of these are packaged into container. you can run the container everywhere. 
therefore, docker solved the problem of environment inconsistency
3.2) Protection of system resources available to certain applications: Application running slow because multiple application running in the same server, the problem in another applications such as memory leakage, poor algorithm slowing down other applications.  
3.3) Scalability: Christmas promotion is coming, our server will not be able to support the traffic..., but it doesn't make sense to buy 10 more server just for the a few days of the year. By using docker, ops can increase the server from 10 to 100, 1000 by one click!
Docker can be used to quicky scale an application. 

4. Key concepts:
- image = the package of the application
- repository = the place store the application
- container = the place to run the application
- build image; pull image from repository, run image as container 
- pull "mirror" from "repository", run "mirror" as "container"
- what exactly is image? image are just a bunch of files, such as our applications and application's environments; files are stored by layers(linux kernel -> centos -> jdk -> tomcat -> app (all readonly) -> writable (log, configuration etc.))
- what exactly is container? container is a process. you can think of container as a virtual machine, the image is like all of files in a virtual machine.
- what exactly is repository? hub.docker.com
- 5-2 第一个Docker容器

Docker Engine
Docker Image（镜像）
Docker Container（容器）
Docker Registry（仓库）

3. Use cases

4. Simple examples
4.1 Hello World
- docker pull
- docker image
- docker run IMAGENAME

4.2 Nginx
- search nginx in docker repository
- docker pull nginx:latest
- docker image
- docker run -d nginx:latest (run in background)
- docker ps (check running container) 
- docker exec -it CONTAINERID bash (run a command in a running container interactively using a pseudo-TTY)
- execute linux command: ls, which nginx, ps -ef

- docker network
- linux use namespace to isolate resources: PID namespace, file namespace, network namespace
- Bridge (default), Host(shared ip, port with host), None (do not communicate with outside)
- port mapping (from docker container <-> host)
- docker run -d -p 8080:80 (host:container) nginx:latest
- netstat -na | grep 8080
- localhost:8080
- docker stop ID
- docker run -d -P nginx:latest

4.3 Build and run a java web application
- Dockerfile: step-by-step instruction for docker to build an image.
- docker build: execute the steps in Dockerfile 
- download jpress war file: 

- Dockerfile:
from tomcat //use tomcat base image as the starting of building my own image
COPY jpress.war /usr/local/tomcat/webapps  // check tomcat image information: CATALINA_HOME

- docker build -t jpress:latest
- docker run -docker -p 8888:8080 jpress:latest
- netstat -na | grep 8888
- docker run -d -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 MYSQL_DATABASE=jpress mysql:latest

4. Applications:
- Continious integration 
- Scalability of services 
- easy deployment to reduce efforts of operations.

5. Some examples
- Jingdong uses 150K docker instances to support their 618 promotion day.
- Tencent uses more than 10K docker instances to support their business.
- Alibaba uses docker to support their business.

6. use cases:
- multiple docker container log to centralized area

- 在我们的平台上，一台 16 核 32G 内存的虚拟机上，需要跑 500+ 个用户的应用（每个应用的功能可以认为是一个网站 + 一系列的 RESTful API），有两个事情很重要： 资源隔离：比如限制应用最大内存使用量，或者资源加载隔离等。 低消耗：虚拟化本身带来的损耗需要尽量的低

- Docker依赖Linux LXC技术，轻量容器概念，其上实际运行的程序是在宿主机上的，本身不是完整的程序系统，也正是其特别之处。

- Operations perspective:
- what are the cons?
- in docker swarm mode?
- how do we manage docker container centrally? 
- how do we manage the docker log file centrally?
- security?
- stateless vs. stateful applications


Docker vs. Micro-Services

Docker vs. DevOps/Continious Integration


Page 1 - Docker: 
Docker is an open source project to pack, ship, run any application as a lightweight container.
Docker allows us to package an application with all its dependecies into a standardized unit.
You can think of docker as a lightweight VM roughly, but it is different from Docker diagram, but they used different machanisms to achieve virtualization.
- Hypervisor to virtualized a layer of hardware and an indepedent guest OS is installed for each VM
- In contrast, Docker do not have Hypervisor, it has a Docker engine layer, Docker engine use LXC, namespace, control groups mechaniusm to achieve isolation and virtulization
- Since docker doesn't need a full Guest OS, it has a faster startup time and less resource consumptions.
- However, the Docker engine introduces a new layer of technologies, may require new way to monitor and control them, also new way of state management and presistence management (what kind of concerns do the Ops people have?)
- very useful in case of stateless multiple server instance; docker适合批量重复的实例化
- 对比传统虚拟机总结
特性			容器					虚拟机
启动			秒级					分钟级
硬盘使用		一般为 MB			一般为 GB
性能			接近原生				弱于
系统支持量	单机支持上千个容器	一般几十个

Page 2 - Docker Architecture
Architecture diagram
three flows

Page 3 - Key concepts
- container
- image
- registry
- 

Page 3 - Demo #1 - docker basics
- docker run ubuntu echo hello docker //we ran a unbuntu and echo a 'hello docker' message
- docker run nginx - p 8080:80 -d nginx //run an nginx application
- curl localhost:8080 //test nginx server
- docker cp index.html CONTAINERID://usr/share/nginx/html //copy index.html from host into nginx
- docker stop CONTAINERID	//stop a container
- docker commit -m 'fun' CONTAINERID nginx-fun //create a new nginx server based on existing images
- docker images //show all the images
- docker rmi IMAGEID //remove an image
- docker ps //display running container
- docker ps -a //display all running container including not running ones
- docker rm CONTAINERID1 CONTAINERID2...

Page 4 - Demo #2 - building docker image using dockerfile
- dockerfile //it is like a linux script providing step-by-step instructions of creating a docker

#Case#1 - Hello World
- FROM alpine:latest #build from base image, like a base class in Java
- MAINTAINER dwen #author is dwen
- CMD echo 'hello dockerfile' #run echo 'hello dockerfile' in alpine, alpine is a version of linux created by docker
- touch Dockerfile
FROM alpine:latest
MAINTAINER dwen
CMD echo "I am building a docker image using dockerfile"
- docker build -t hello_docker . 	#build an image from Dockerfile from current directory
- docker images
- docker run hello_docker

#Case#2 - Nginx server
- touch Dockerfile
- vim Dockerfile
FROM ubuntu  		#build from base image of ubuntu
MAINTAINER dwen     #authur is dwen
RUN apt-get update  #update ubuntu using apt-get
RUN apt-get install -y nginx   	#install nginx using apt-get
COPY index.html /var/www/html   #copy html files to /var/www/html
ENTRYPOINT ["/usr/sbin/nginx", "-g", "daemon off;"] 		#program entrypoint, run nginx
EXPOSE 80			#expose a port as http server
- touch index.html; vi index.html 
- docker build -t sentaca/hello-nginx .
- docker run -d -p 80:80 sentaca/hello-nginx 
- curl http://localhost

Page 5 - layered of mirrors
- layered approach 
- container layer can be RW
- image layer are read-only
- layered approach can improve reuse

Page 6 - Volume
- Volumn can be used to persist data outside and outlived the containers
- e.g. actual data of a database, log file etc
- can be used to shared data across container

#1 mounting a local folder to container
- mkdir html
- cd html && touch index.html && echo "mouting a volumn into docker container" > index.html
- docker run -d -p 80:80 --name sentaca-nginx -v $PWD/html:/usr/share/nginx/html nginx #mount a local folder to the container, $PWD always point to current local directory in host

#2 create a container just for data and the container can be mounted by multiple continers so that data can be shared
- docker create =v $PWD/data:/var/mydata --name data_container ubuntu #create a data container based on ubuntu
- docker run -it --volumn-from data_container ubuntu /bin/bash
- #mount
- #cd /var/mydata
- #touch whatever.txt
- #exit
- ls

Page 7 - Registry
- Host: our pc
- image: package of files that be used to create container
- container: an running instance of images
- registry: repository of images
- daemon: the process running in the background
- client: the command line interface to enter commands

- docker search mysql
- docker pull mysql
- docker tag mysql sentaca/whalesay
- docker login (enter username, password)
- docker push sentaca/mysql
- go to docker hub to search for image and look for developer's guides

Page 8 - Docker Compose 项目
- Docker Compose 是 Docker 官方编排（Orchestration）项目之一, it is a tool of defining and running multi-container Docker applications. e.g. running nginx, java web app, mysql at the same time
- work with one container host and run a container or two, which is great for testing or local development.
- docker-compose --version
- picture: user->nginx->web application->mysql
- 3 containers for nginx, web app, mysql
- create three folders: webapp, nginx, data
- use Dockerfile to build images for webapp, nginx, data
- create a compose file: docker-compose.yaml
- docker-compose.yaml specify a network for the services of nginx, build each webapp and db specify the dependency between the containers in the group

version: '3'
networks:
	ghost:

	services:
		ghost-app:
		build: ghost   (build from ghost )
		networks:
			- ghost
		depends_on:
			- db
		ports:
			-"2368:2368"

	nginx:
		build: nginx
		networks:
			- ghost
		depends_on:
			- ghost-app
		ports:
			- "80:80"


	db:
		image: "mysql:5.7.15" (pull from repository)
		networks:
			- ghost
		environment:
			MYSQL_ROOT_PASSWORD: mysqlroot
			MYSQL_USER: ghost
			MYSQL_PASSWORD: ghost
		volumns: 
			- $PWD/data:/var/lib/mysql 	(mount data folder, so that db data can outlive the container)
			- ports: "3306:3306"

- docker-compose up -d
- docker-compose stop
- docker-compose rm
- docker-compose build

Page 9 - Docker Machine 是 Docker 官方编排（Orchestration）项目之一，负责在多种平台上快速安装 Docker 环境。
- https://yeasy.gitbooks.io/docker_practice/content/machine/ picture

Install and run Docker on Mac or Windows
Provision and manage multiple remote Docker hosts
Provision Swarm clusters

Page 10 - Docker Swarm
- Docker Swarm 是 Docker 官方三剑客项目之一，提供 Docker 容器集群服务，是 Docker 官方对容器云生态进行支持的核心方案。
- 使用它，用户可以将多个 Docker 主机封装为单个大型的虚拟 Docker 主机，快速打造一套容器云平台。
- Swarm 是使用 SwarmKit 构建的 Docker 引擎内置（原生）的集群管理和编排工具。
- 包括Docker Swarm（译者注：Docker集群工具）、Docker Machine（译者注：Docker管理工具）以及Docker Compose（译者注：Docker编排工具）。


Page 11 - Docker 
- Swarm is docker's cluster management and orchestration features
- With Docker Swarm we’re now going to turn that small test environment into a larger setup of clustered container hosts that can be used to scale your operations into something even more useful. This is a bit more advanced and will involve things like service discovery, clustering, and remote management.
- A swarm consists of multiple Docker hosts which run in swarm mode and act as managers (to manage membership and delegation) and workers (which run swarm services).
- When you create a service, you define its optimal state (number of replicas, network and storage resources available to it, ports the service exposes to the outside world, and more). Docker works to maintain that desired state. 
-  A task is a running container which is part of a swarm service and managed by a swarm manager, as opposed to a standalone container.

- use tools to set up container hosts (Machine), 
Docker Machine is a tool that lets you install Docker Engine on virtual hosts, and manage the hosts with docker-machine commands. You can use Machine to create Docker hosts on your local Mac or Windows box, on your company network, in your data center, or on cloud providers like Azure, AWS, or Digital Ocean.
- manage multiple containers linked together (Compose), 
- and treating your container hosts as a cluster (Swarm).

Using docker-machine commands, you can start, inspect, stop, and restart a managed host, upgrade the Docker client and daemon, and configure a Docker client to talk to your host.

Page 11 - Mesos、Kubernetes 

If you work primarily on an older Mac or Windows laptop or desktop that doesn’t meet the requirements for the new Docker for Mac and Docker for Windows apps, then you need Docker Machine in order to “run Docker” (that is, Docker Engine) locally. Installing Docker Machine on a Mac or Windows box with the Docker Toolbox installer provisions a local virtual machine with Docker Engine, gives you the ability to connect it, and run docker commands.

Docker Engine runs natively on Linux systems. If you have a Linux box as your primary system, and want to run docker commands, all you need to do is download and install Docker Engine. However, if you want an efficient way to provision multiple Docker hosts on a network, in the cloud or even locally, you need Docker Machine.


- Managers and workers: A swarm consists of multiple Docker hosts which run in swarm mode and act as managers (to manage membership and delegation) and workers (which run swarm services).
- Services: When you create a service, you define its optimal state (number of replicas, network and storage resources available to it, ports the service exposes to the outside world, and more). Docker works to maintain that desired state. 
- Task: A task is a running container which is part of a swarm service and managed by a swarm manager, as opposed to a standalone container.

- Node: A node is an instance of the Docker engine participating in the swarm. production swarm deployments typically include Docker nodes distributed across multiple physical and cloud machines.
- Manager Node: Manager nodes can be used to deployed application and dispatch task to worker nodes. They also perform the orchestration and cluster management functions required to maintain the desired state of the swarm. 
- Worker Node: Worker nodes receive and execute tasks dispatched from manager nodes. 
- Services: A service is the definition of the tasks to execute on the manager or worker nodes.
- replicated services: the swarm manager distributes a specific number of replica tasks among the nodes based upon the scale you set in the desired state.
- global services: the swarm manager runs one task for the service on every available node in the cluster. 
- Task: A task carries a Docker container and the commands to run inside the container. It is the atomic scheduling unit of swarm. 
- Task: 

docker swarm init --advertise-addr $(hostname -i)

Creating services

docker service create -p 80:80 --name web nginx:latest
docker service ls

inspect the service:

docker service inspect web

docker service scale web=15
Docker has spread the 15 services evenly over all of the nodes

docker service ps web

docker node update --availability drain node2

docker stack deploy --compose-file docker-compose.yml vossibility


#Docker container networking
- docker network ls
- 
http://training.play-with-docker.com/docker-networking-hol/#task1
- The docker network command is the main command for configuring and managing container networks.
- Default networks: When you install Docker, it creates three networks automatically. You can list these networks using the docker network ls command.
- These three networks are built into Docker. When you run a container, you can use the --network flag to specify which networks your container should connect to.
- The bridge network represents the docker0 network present in all Docker installations. Unless you specify otherwise with the docker run --network=<NETWORK> option, the Docker daemon connects containers to this network by default.

- only one Linux kernel in Docker
- Docker over virtual machine
- Container from AWS
- network: network namespace
- dockerize applications
- VAS team, people are mostly from operations
- most of applications running on Docker
- some VM
- CI pipeline
- plugin in Jenkins
- registry, pull from staging
- swarm, charging. Docker is used.

- what are Docker being used for? for development/qa and/or production?
they have 2 VMs, almost every applications are dockerized.
- what kind of applications are currently supported by docker? web tier, application tier, database tier or any business areas specifically?
same as above
- what are the scale of the docker container deployment? 10s, 100s, or 1000s?
2 instances of docker, one on each VM
- what kind of orchestration tool do they use? Docker Swarm, Kubernetes, Meso...?
Charging team uses Docker swarm
- do they use private registry to host their own images?
yes. they have a CI pipeline: pull from git repository, move into Jenkins, Jenkins will use docker plugin to create image and push the image into repository. Operations can pull the image from the server.
- do they have any corporate standard on virtualization technology? what are the direction?
they used both VM and docker.
- do they have any specific concerns (security, maturity of the technology, complexity...)?
not really.

- what base image do we want to use?
tomcat, jdk or linux (and add JDK and Tomcat)

#Dockerfile
- FROM 是必备的指令，并且必须是第一条指令。
- 在 Docker Store 上有非常多的高质量的官方镜像
- 有可以直接拿来使用的服务类的镜像，如 nginx、redis、mongo、mysql、httpd、php、tomcat 等；
- 也有一些方便开发、构建、运行各种语言应用的镜像，如 node、openjdk、python、ruby、golang 等。
- RUN 指令是用来执行命令行命令的。由于命令行的强大能力，RUN 指令在定制镜像时是最常用的指令之一。 有两种格式，一种类似于命令行，一种类似于函数调用。
- RUN echo '<h1>Hello, Docker!</h1>' > /usr/share/nginx/html/index.html
- RUN ["可执行文件", "参数1", "参数2"]
- COPY package.json /usr/src/app/
- COPY hom* /mydir/
- COPY hom?.txt /mydir/
- ADD 指令和 COPY 的格式和性质基本一致。但是在 COPY 基础上增加了一些功能
- 比如 <源路径> 可以是一个 URL，这种情况下，Docker 引擎会试图去下载这个链接的文件放到 <目标路径> 去。
- 所以不如直接使用 RUN 指令，然后使用 wget 或者 curl 工具下载，处理权限、解压缩、然后清理无用文件更合理。因此，这个功能其实并不实用，而且不推荐使用。
- 在 Docker 官方的 Dockerfile 最佳实践文档 中要求，尽可能的使用 COPY，因为 COPY 的语义很明确
- CMD 指令的格式和 RUN 相似，也是两种格式： shell 格式：CMD <命令>; exec 格式：CMD ["可执行文件", "参数1", "参数2"...]
- CMD echo $HOME -> CMD [ "sh", "-c", "echo $HOME" ]
- ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数。ENTRYPOINT 在运行时也可以替代，不过比 CMD 要略显繁琐，需要通过 docker run 的参数 --entrypoint 来指定。


1. solution we build before
2. WBS
3. rpm put into linux
4. 

Git lab (AWS)
ci server (AWS)

- Docker plugin


2 VMs
- VM#1: WBS(Spring MVC) + SMS Simulator 1 + SMS Simulator 2 
- VM#2: MySQL database

2.1 need to understand basic functionalities of WBS and SMS Simulator
2.2 build a Spring MVC application image
2.3 build a SMS Simulator image
2.4 use docker compose to bring up a WBS and 2 sms simulator
java -classpath simple-1.0-SNAPSHOT.jar org.atwjsw.javaapp.App
https://codefresh.io/docker-guides/dockerize-java-application/
2.5 integrate with Jenkins
2.6 export import docker image without using registry
http://blog.csdn.net/a906998248/article/details/46236687


ENTRYPOINT ["java","-classpath","app.jar","org.atwjsw.javaapp.EchoServer", "5001"]

ENTRYPOINT ["java","-classpath","app.jar","org.atwjsw.javaapp.EchoClient", "server", "5001"]

docker network create client_server_network

docker run --network-alias server --network client_server_network -it echo-server

docker run --network client_server_network -it echo-client

#Enter a container
docker exec -it test-wordpress bash

#if two container wants to talk to each other, they need to be on the same logical network. (overlay network??)
#if docker compose, sepcify same network and --network-alias server
# how to run build mysql image

#MySQL in a container?
- version?
docker run --name=test-mysql --env="MYSQL_ROOT_PASSWORD=mypassword" mysql
docker logs test-mysql
docker inspect test-mysql  //"IPAddress": "172.17.0.2",
docker rm -f test-mysql //stop and remove container
docker run --detach --name=test-mysql --env="MYSQL_ROOT_PASSWORD=mypassword" --publish 6603:3306 mysql
mysql -uroot -pmypassword -h 127.0.0.1 -P 6603

CREATE DATABASE testDB;

USE testDB;

CREATE TABLE Persons (
    PersonID int,
    LastName varchar(255),
    FirstName varchar(255),
    Address varchar(255),
    City varchar(255) 
);

INSERT INTO Persons (PersonID, LastName, FirstName, Address, City) VALUES (1, 'Daniel', 'Wen', '267 William Graham', 'Aurora');
INSERT INTO Persons (PersonID, LastName, FirstName, Address, City) VALUES (2, 'Andrew', 'Wen', '267 William Graham', 'Aurora');
INSERT INTO Persons (PersonID, LastName, FirstName, Address, City) VALUES (3, 'Jonathan', 'Wen', '267 William Graham', 'Aurora');
INSERT INTO Persons (PersonID, LastName, FirstName, Address, City) VALUES (4, 'Michelle', 'Wen', '267 William Graham', 'Aurora');

INSERT INTO testtab (name) VALUES ('Daniel Wen');
INSERT INTO testtab (name) VALUES ('Andrew Wen');
INSERT INTO testtab (name) VALUES ('Jonathan Wen');

records will only be available in the docker be ran, if the docker is removed, it will be gone;

#MySQL Data storage??
- create a new directory
mkdir C:\Users\wenda\Documents\storage\docker\mysql-datadir

- docker run -d --name=new-mysql --env="MYSQL_ROOT_PASSWORD=mypassword" -p 6603:3306 -v C:\Users\wenda\Documents\storage\docker\mysql-datadir:/var/lib/mysql mysql

#MySQL schema built into image??
FROM mysql
COPY database.sql /...
COPY conf /...conf
CMD [database.sql]
EXPOSE: 3306

### MySQL approach
1) mkdir docker/mysql

2) copy schema.sql to vm
#create database test;
use test;
CREATE TABLE testtab
(
id INTEGER AUTO_INCREMENT,
name TEXT,
PRIMARY KEY (id)
) COMMENT='this is my test table';

3) copy Dockerfile to vm
FROM mysql:5.7.15
MAINTAINER daniel.wen@sentaca.com
ENV MYSQL_DATABASE=test MYSQL_ROOT_PASSWORD=123456
VOLUME /var/lib/mysql
ADD schema.sql /docker-entrypoint-initdb.d
EXPOSE 3306

4) docker volume create mysql_data_volume
5) docker build -t wbs-mysql .
[Warning] IPv4 forwarding is disabled. Networking will not work.
5) docker run -d --name wbs-mysql -v mysql_data_volume:/var/lib/mysql -p 3306:3306 wbs-mysql

6) yum install mysql

7) mysql -uroot -ppassword -h 127.0.0.1 -P 3306
mysql -uroot -ppassword -h 192.168.246.203 -P 3306
mysql -h 127.0.0.1 -P 3306 -uroot -ppassword -e "show databases;"

8) mysql allow remote access
https://www.imooc.com/article/16606
CentOS7开启MySQL远程访问
http://blog.csdn.net/u014066037/article/details/55194802
1、关闭防火墙：sudo systemctl stop firewalld.service
2、关闭开机启动：sudo systemctl disable firewalld.service
3、安装iptables防火墙
执行以下命令安装iptables防火墙：sudo yum install iptables-services
4、配置iptables防火墙，打开指定端口（CentOS6一样）
5、设置iptables防火墙开机启动：sudo systemctl enable iptables
6. 修改防火墙配置文件
vi /etc/sysconfig/iptables 
*filter
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
-A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
-A INPUT -p icmp -j ACCEPT
-A INPUT -i lo -j ACCEPT	#允许本机访问
-A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT
-A INPUT -m state --state NEW -m tcp -p tcp --dport 3306 -j ACCEPT
-A INPUT -j REJECT --reject-with icmp-host-prohibited
-A FORWARD -j REJECT --reject-with icmp-host-prohibited
7. 重新加载规则
service iptables restart  

9) test connection: mysql -h 192.168.246.203 -P 3306 -uroot -ppassword -e "show databases;"

TO DO: 
1. test on local virtual machine
2. test on AWS VM

mysql container + data container

#先检查是否安装了iptables 
service iptables status 
#安装iptables 
yum install -y iptables 
#升级iptables 
yum update iptables 
#安装iptables-services 
yum install iptables-services

1.开放mysql访问端口3306


# Add VOLUMEs to allow backup of config, logs and databases
VOLUME  ["/etc/postgresql", "/var/log/postgresql", "/var/lib/postgresql"]

systemctl start docker.service

### docker-compose WBS + simulator
1） With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration.
2）Using Compose is basically a three-step process:
- Define your app’s environment with a Dockerfile so it can be reproduced anywhere.
- Define the services that make up your app in docker-compose.yml so they can be run together in an isolated environment.
- Lastly, run docker-compose up and Compose will start and run your entire app. 

Steps
1）create 2 directroies
mkdir echoclient
mkdir echoserver
2) create Dockerfile in echoclient
cd echoclient
vi Dockerfile
3) create Dockerfile in echoserver
4) vi docker-compose.yml
version: '2'
networks:
	echo:
services:
	echo-client:
		build: echoclient
		networks:
			- echo
		depends_on:
			- echoserver
		ports:
			- "5000:5000"

	echo-server
		build: echoserver
		networks:
			- echo
5) docker-compose up -d
6) docker-compse stop
7) docker-compose rm
8) docker-compose build


Docker原理
1. docker是linux系统上的，虽然官网提供了windows和MacOS版本的安装包，但它们都是靠虚拟机或类似的技术支撑的。
2. Cgroups：Cgroups是Linux内核功能，它让两件事情变成可能：限制Linux进程组的资源占用（内存、CPU）；为进程组制作 PID、UTS、IPC、网络、用户及装载命名空间。
3. Union文件系统：在union文件系统里，文件系统可以被装载在其他文件系统之上，其结果就是一个分层的积累变化。





 



